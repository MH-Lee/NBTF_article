{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import collections \n",
    "\n",
    "from ast import literal_eval\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import namedtuple\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from  scipy import spatial\n",
    "from sklearn.preprocessing import scale, robust_scale, minmax_scale, maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec.load('../model/word_embedding/Doc2vec_new_small2_4.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list  = pd.read_excel('./new_data/final_list_score3.xlsx')\n",
    "inv_list  = pd.read_excel('./new_data/invest_df.xlsx')\n",
    "final_list = final_list[final_list['established_date2'] < 5.0].reset_index(drop=True)\n",
    "final_list.fillna('nan', inplace=True)\n",
    "final_list['token'] = final_list.token.apply(lambda x:literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list['invest'] = [str(x).split(',') for x in final_list['invest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest = list(set([el.strip() for el in final_list['invest'].sum()]))\n",
    "inv_list.set_index('invest', inplace=True)\n",
    "inv_list = inv_list.apply(minmax_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inv_list = inv_list.where(inv_list > np.quantile(inv_list['invest_num'], 0.25), np.quantile(inv_list['invest_num'], 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_id = {k:v for k, v in enumerate(d2v_model.docvecs.doctags.keys())}\n",
    "doc_vec = {paragraph_id[k]:list(v) for k,v in enumerate(d2v_model.docvecs.vectors_docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Indiegogo\n",
      "이재권\n",
      " 현대자동차 제로원엑셀러레이터\n",
      " 신용보증기금 경기스타트업지점\n"
     ]
    }
   ],
   "source": [
    "inv_tech_df = pd.DataFrame(columns = ['investor', 'tech', 'weights', 'company'])\n",
    "for values_list in  final_list[['company', 'predicted_small_label1', 'predicted_small_label2', 'invest']].values:\n",
    "#     print(values_list)\n",
    "    for com in values_list[-1]:\n",
    "#         print(com)\n",
    "        if com.strip() in ['nan', '비공개', '크라우드펀딩', '사모펀드']:\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                weights = inv_list.loc[com.strip()].values[0]\n",
    "                inv_tech_df = inv_tech_df.append({'investor':com.strip(), 'tech':values_list[1], 'weights':weights, 'company':values_list[0]}, \\\n",
    "                                                 ignore_index=True)\n",
    "            except KeyError:\n",
    "                print(com)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = list(inv_list.index)\n",
    "inv_idx = [\"i%d\" % i for i in range(1,len(inv_list.index)+1)]\n",
    "inv_node = pd.DataFrame({'node':inv,'node_idx':inv_idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_tech_df.rename(columns={'investor':'node'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_tech_df.to_excel('./new_data/new_experiment/investor_graph2.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_score(G) :\n",
    "    tt =  { \"degree\":dict(G.degree()), \n",
    "            \"degree_centrality\":nx.degree_centrality(G), #normalized degree, degree가 클수록 높음\n",
    "            \"closeness_centrality\":nx.closeness_centrality(G), \n",
    "            \"betweenness_centrality\":nx.betweenness_centrality(G), \n",
    "            \"pagerank\":nx.pagerank(G)}\n",
    "    tt_df = pd.DataFrame(tt)\n",
    "    tt_df['node'] = list(tt_df.index)\n",
    "    tt_df = pd.merge(tt_df,node_df,how='left')\n",
    "    return tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_label = list(set(final_list.predicted_small_label1.tolist() + final_list.predicted_small_label2.tolist()))\n",
    "comb_dv = list(itertools.combinations(small_label,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tech_similarity_list = list()\n",
    "tech_mat = pd.DataFrame(index=small_label, columns=small_label)\n",
    "for comb1, comb2 in comb_dv:\n",
    "    cos_sim = cosine_similarity([doc_vec[comb1]], [doc_vec[comb2]])[0][0]\n",
    "    tech_mat.loc[comb1,comb2] = cos_sim\n",
    "    tech_mat.loc[comb2,comb1] = cos_sim\n",
    "    tech_similarity_list.append(cos_sim)\n",
    "tech_mat2 = tech_mat.where(tech_mat > 0.13, 0.0)\n",
    "tech_mat3 = tech_mat2.where(tech_mat2 == 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_edge_df = pd.DataFrame(columns= ['source', 'target', 'weights'])\n",
    "for idx, comb in enumerate(comb_dv):\n",
    "    edge =tech_mat3.loc[comb[0], comb[1]]\n",
    "    if edge == 0.0:\n",
    "        continue\n",
    "    else:\n",
    "        weight = tech_mat2.loc[comb[0], comb[1]]\n",
    "        t_edge_df = t_edge_df.append({'source':comb[0], 'target':comb[1], 'weights':weight}, ignore_index=True)\n",
    "t_edge = [tuple(element) for element in t_edge_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테라 메디블록\n",
      "헬스메디 서큘러스\n",
      "뉴포트파트너스 닥터다이어리\n",
      "머니브레인 플러스티브이\n",
      "송아리아이티 메디픽셀\n",
      "콴텍 캐치잇플레이\n",
      "스켈터랩스 스테이지랩스\n",
      "메디픽셀 아모랩\n",
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "company = list(final_list['company'])\n",
    "company_idx = [\"c%d\" % i for i in range(1,len(company)+1)]\n",
    "node_df = pd.DataFrame({'node':company,'node_idx':company_idx, 'token':final_list.token.tolist()})\n",
    "sim_sentence = pd.DataFrame(columns=company, index=company)\n",
    "comb_c = list(itertools.combinations(node_df.node.tolist(),2))\n",
    "similarity_list = list()\n",
    "for idx,comb in enumerate(comb_c):\n",
    "    if idx % 100 == 0:\n",
    "        print(comb[0], comb[1])\n",
    "    sent1 = node_df.loc[node_df.node == comb[0], 'token'].values[0]\n",
    "    sent2 = node_df.loc[node_df.node == comb[1], 'token'].values[0]\n",
    "    sent1_vector = d2v_model.infer_vector(sent1)\n",
    "    sent2_vector = d2v_model.infer_vector(sent2)\n",
    "    sim_sentence.loc[comb[0], comb[1]] = cosine_similarity([sent1_vector], [sent2_vector])[0][0]\n",
    "    sim_sentence.loc[comb[1], comb[0]] = cosine_similarity([sent2_vector], [sent1_vector])[0][0]\n",
    "    similarity_list.append(cosine_similarity([sent1_vector], [sent2_vector])[0][0])\n",
    "sim_sentence2 = sim_sentence.where(sim_sentence > np.quantile(similarity_list, 0.90), 0.0)\n",
    "sim_sentence3 = sim_sentence2.where(sim_sentence2 == 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = pd.DataFrame(columns= ['source', 'target', 'weights'])\n",
    "for idx,comb in enumerate(comb_c):\n",
    "    edge =sim_sentence3.loc[comb[0], comb[1]]\n",
    "    if edge == 0.0:\n",
    "        continue\n",
    "    else:\n",
    "        weight = sim_sentence2.loc[comb[0], comb[1]]\n",
    "        edge_df = edge_df.append({'source':comb[0], 'target':comb[1], 'weights':weight}, ignore_index=True)\n",
    "c_edge = [tuple(element) for element in edge_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = list(set(list(final_list['predicted_small_label1']) + list(final_list['predicted_small_label2'])))\n",
    "small_idx = [\"s%d\" % i for i in range(1,len(small)+1)]\n",
    "small_node = pd.DataFrame({'node':small,'node_idx':small_idx})\n",
    "node_df = node_df.append(pd.DataFrame(data=small_node),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_edge = []\n",
    "for i in range(len(final_list)):\n",
    "    company = final_list['company'][i]\n",
    "    small1 = final_list['predicted_small_label1'][i]\n",
    "    if small1 == 'nan':\n",
    "        print(company)\n",
    "    else:\n",
    "        s_edge.append((small1, company,1))\n",
    "    small2 = final_list['predicted_small_label2'][i]\n",
    "    if small2 == 'nan':\n",
    "        print(company)\n",
    "    else:\n",
    "        s_edge.append((small2, company,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmhoon012\\Anaconda3\\envs\\project\\lib\\site-packages\\pandas\\core\\frame.py:4304: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "company_tech = final_list[['company','predicted_label1','predicted_label2', 'predicted_small_label1', 'predicted_small_label2']]\n",
    "company_tech.rename(columns={'company':'node'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = node_df.append(pd.DataFrame(data=inv_node),ignore_index=True)\n",
    "node_df.to_excel('./new_data/new_experiment/new_node_df.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_edge = [(el[0], el[3],el[2]) for el in inv_tech_df.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph(T-C-I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nondirect_G = nx.Graph()\n",
    "nondirect_G.add_weighted_edges_from(c_edge+s_edge+i_edge)\n",
    "nondirect_score = network_score(nondirect_G)\n",
    "nondirect_score.to_excel('./new_data/new_experiment/network_score2.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_network_result = nondirect_score[nondirect_score['node_idx'].isin(company_idx)].sort_values([\"pagerank\"], ascending=[False])\n",
    "company_network_result['total_weight'] = company_network_result[['closeness_centrality','betweenness_centrality', 'pagerank']].sum(axis=1)\n",
    "company_network_result = company_network_result.sort_values([\"total_weight\"], ascending=[False]).reset_index(drop=True)\n",
    "company_network_result = pd.merge(company_network_result, company_tech)\n",
    "company_network_result.to_excel('./new_data/new_experiment/company_network_result_new.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_score1 = company_network_result.groupby('predicted_small_label1').mean()[['total_weight']].sort_values('total_weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_score2 = company_network_result.groupby('predicted_small_label2').mean()[['total_weight']].sort_values('total_weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_score2.rename(columns ={'total_weight':'total_weight2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_index = list(set(com_score1.index.tolist() + com_score2.index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_total_df = pd.DataFrame(index=total_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_score1.index.name = None\n",
    "com_score2.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_total_df = company_total_df.join(com_score1)\n",
    "company_total_df = company_total_df.join(com_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_total_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_score = pd.DataFrame(company_total_df.mean(axis=1), columns=['total_weight']).sort_values('total_weight', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emerging Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_network_result = nondirect_score[nondirect_score['node_idx'].isin(small_idx)].sort_values([\"pagerank\"], ascending=[False])\n",
    "tech_network_result['total_weight'] = tech_network_result[['closeness_centrality','betweenness_centrality', 'pagerank']].sum(axis=1)\n",
    "tech_network_result = tech_network_result.sort_values([\"total_weight\"], ascending=[False]).reset_index(drop=True)\n",
    "tech_network_result.to_excel('./new_data/new_experiment/tech_network_result_new.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_network_result[['node', 'total_weight']].sort_values('total_weight', ascending=False).to_excel('emerging_tech.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### investor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_network_result = nondirect_score[nondirect_score['node_idx'].isin(inv_idx)].sort_values([\"pagerank\"], ascending=[False])\n",
    "inv_network_result['total_weight'] = inv_network_result[['closeness_centrality','betweenness_centrality', 'pagerank']].sum(axis=1)\n",
    "inv_network_result = inv_network_result.sort_values([\"total_weight\"], ascending=[False]).reset_index(drop=True)\n",
    "inv_network_result = pd.merge(inv_network_result, inv_tech_df)\n",
    "inv_network_result.to_excel('./new_data/new_experiment/inv_network_result_new.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_score = inv_network_result.groupby('tech').mean()[['total_weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_score.sort_values('total_weight', ascending=False).to_excel('inv_tech.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph1(C-T, C-C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nondirect_G = nx.Graph()\n",
    "nondirect_G.add_weighted_edges_from(c_edge+s_edge)\n",
    "nondirect_score = network_score(nondirect_G)\n",
    "nondirect_score.to_excel('./new_data/new_experiment/network_score.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NBTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_network_result = nondirect_score[nondirect_score['node_idx'].isin(company_idx)].sort_values([\"pagerank\"], ascending=[False])\n",
    "company_network_result['total_weight'] = company_network_result[['closeness_centrality','betweenness_centrality', 'pagerank']].sum(axis=1)\n",
    "company_network_result = company_network_result.sort_values([\"total_weight\"], ascending=[False]).reset_index(drop=True)\n",
    "company_network_result = pd.merge(company_network_result, company_tech)\n",
    "company_network_result.to_excel('./new_data/new_experiment/company_network_result.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emerging Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_network_result = nondirect_score[nondirect_score['node_idx'].isin(small_idx)].sort_values([\"pagerank\"], ascending=[False])\n",
    "tech_network_result['total_weight'] = tech_network_result[['closeness_centrality','betweenness_centrality', 'pagerank']].sum(axis=1)\n",
    "tech_network_result = tech_network_result.sort_values([\"total_weight\"], ascending=[False]).reset_index(drop=True)\n",
    "tech_network_result.to_excel('./new_data/new_experiment/tech_network_result.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph2(C-T, C-C, T -T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nondirect_G2 = nx.Graph()\n",
    "nondirect_G2.add_weighted_edges_from(c_edge+s_edge+t_edge)\n",
    "nondirect_score2 = network_score(nondirect_G2)\n",
    "nondirect_score2.to_excel('./new_data/new_experiment/network_score2.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NBTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_network_result2 = nondirect_score2[nondirect_score2['node_idx'].isin(company_idx)].sort_values([\"pagerank\"], ascending=[False])\n",
    "company_network_result2['total_weight'] = company_network_result2[['closeness_centrality','betweenness_centrality', 'pagerank']].sum(axis=1)\n",
    "company_network_result2 = company_network_result2.sort_values([\"total_weight\"], ascending=[False]).reset_index(drop=True)\n",
    "# company_network_result2 = pd.merge(company_network_result2, company_tech)\n",
    "company_network_result2.to_excel('./new_data/new_experiment/company_network_result2.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emerging Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_network_result2 = nondirect_score2[nondirect_score2['node_idx'].isin(small_idx)].sort_values([\"pagerank\"], ascending=[False])\n",
    "tech_network_result2['total_weight'] = tech_network_result2[['closeness_centrality','betweenness_centrality', 'pagerank']].sum(axis=1)\n",
    "tech_network_result2 = tech_network_result2.sort_values([\"total_weight\"], ascending=[False]).reset_index(drop=True)\n",
    "tech_network_result2.to_excel('./new_data/new_experiment/tech_network_result2.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph3 ( I - T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nondirect_G3 = nx.Graph()\n",
    "nondirect_G3.add_weighted_edges_from(i_edge)\n",
    "nondirect_score3 = network_score(nondirect_G3)\n",
    "nondirect_score3.to_excel('./new_data/new_experiment/investor_network_score.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emerging Technology using investor node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_network_result = nondirect_score3[nondirect_score3['node_idx'].isin(small_idx)].sort_values([\"pagerank\"], ascending=[False])\n",
    "invest_network_result['total_weight'] = invest_network_result[['closeness_centrality','betweenness_centrality', 'pagerank']].sum(axis=1)\n",
    "invest_network_result = invest_network_result.sort_values([\"total_weight\"], ascending=[False]).reset_index(drop=True)\n",
    "invest_network_result.to_excel('./new_data/new_experiment/invest_network_result.xlsx', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 그리키"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = []\n",
    "for i in range(len(final_list)):\n",
    "    weight_sum = 0 \n",
    "    for j in range(len(final_list['invest'][i])):\n",
    "        if final_list['invest'][i][j] != 'nan':\n",
    "            company = final_list['company'][i]\n",
    "            invest = final_list['invest'][i][j]\n",
    "            company = node_df[node_df['node']==company]['node_idx'].values[0]\n",
    "            invest = node_df[node_df['node']==invest]['node_idx'].values[0]\n",
    "            edge.append((company,invest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_edge = []\n",
    "for i in range(len(final_list)):\n",
    "    company = final_list['company'][i]\n",
    "    small1 = final_list['predicted_small_label1'][i]\n",
    "    small2 = final_list['predicted_small_label2'][i]\n",
    "    small1 = node_df[node_df['node']==small1]['node_idx'].values[0]\n",
    "    small2 = node_df[node_df['node']==small2]['node_idx'].values[0]\n",
    "    company = node_df[node_df['node']==company]['node_idx'].values[0]\n",
    "    s_edge.append((small1,company))\n",
    "    s_edge.append((small2,company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_edge = pd.DataFrame(s_edge,columns=['source','target'])\n",
    "edge = pd.DataFrame(edge,columns=['source','target'])\n",
    "df_edges = s_edge.append(edge, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes = pd.DataFrame()\n",
    "df_nodes['name'] = node_df['node_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = []\n",
    "for i in range(len(df_nodes)):\n",
    "    if df_nodes['name'][i][0] == 'c' :\n",
    "        group.append(0)\n",
    "    elif df_nodes['name'][i][0] == 's' :\n",
    "        group.append(1)\n",
    "    elif df_nodes['name'][i][0] == 'i' :\n",
    "        group.append(2)     \n",
    "        \n",
    "df_nodes['group'] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nondirect_score[['node_idx','pagerank']].copy()\n",
    "a.columns = ['name','nodesize']\n",
    "df_nodes = pd.merge(df_nodes,a,how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(df_nodes)):\n",
    "    if df_nodes['group'][i] == 0:\n",
    "        temp.append(df_nodes['nodesize'][i]*10000)\n",
    "    else :\n",
    "        temp.append(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes['nodesize'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes['nodesize'] = df_nodes['nodesize']*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {0:'#ffdf55', 1:'#eebcbc', 2:'#72bbd0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(df_nodes['group'])\n",
    "classnames, indices = np.unique(classes, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph(day=\"Stackoverflow\")\n",
    "\n",
    "for index, row in df_nodes.iterrows():\n",
    "    G.add_node(row['name'], group=row['group'], nodesize=row['nodesize'])\n",
    "    \n",
    "for index, row in df_edges.iterrows():\n",
    "    G.add_edges_from([(row['source'], row['target'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,25))\n",
    "options = {\n",
    "    'edge_color': '#FFDEA2',\n",
    "    'width': 1,\n",
    "    'with_labels': True,\n",
    "    'font_weight': 'regular',\n",
    "}\n",
    "colors = [color_map[G.nodes[node]['group']] for node in G]\n",
    "sizes = [G.nodes[node]['nodesize']*10 for node in G]\n",
    "\n",
    "nx.draw(G, node_color=colors, node_size=sizes, pos=nx.spring_layout(G, k=0.25, iterations=50), **options)\n",
    "ax = plt.gca()\n",
    "ax.collections[0].set_edgecolor(\"#555555\") \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
